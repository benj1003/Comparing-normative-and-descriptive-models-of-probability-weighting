# Latent mixture model used to:
# 1) Generate data from synthetic agents 
# 2) Do model recovery 
# 3) Do parameter recovery/parameter estimation
#
# Cumulative Prospect Theory (pt) presented by Tversky and Kahneman (1992)
# LML model (lml) presented by Peters et al. (2020)
#
# Note that the model indicator variable z, is parameter expanded to take on 8 different values rather than 2. This
# allows better convergence. It allows 8 different values of the indicator variable to map to each model, such that 
# a z of 1,3,5,7 maps to the pt model, 2,4,6,8 map to the LML model


model{

##LIKELIHOOD
for (g in 1:nGambles){
for (i in 1:nAgents){
for (c in 1:nChunks){
for (t in 1:nTrials){ #eg. nTrials = 100, nChunks = 10 --> 1:10, 11:20, ..., 91:100
	#-----------pt-model-----------
	u_a1_pt[g,i,c,(t-chunkLength*(c-1))]	= pow(dx1[g,i,t],alpha_pt[i,c])							#prospect utility, computed by exponentiating current wealth by alpha   	
	u_a2_pt[g,i,c,(t-chunkLength*(c-1))]	= pow(dx2[g,i,t],alpha_pt[i,c])						 	
	u_b1_pt[g,i,c,(t-chunkLength*(c-1))]	= pow(dx3[g,i,t],alpha_pt[i,c])
	u_b2_pt[g,i,c,(t-chunkLength*(c-1))]	= pow(dx4[g,i,t],alpha_pt[i,c])

	den_w_a1_pt[g,i,c,(t-chunkLength*(c-1))]	= delta_pt[i,c]*pow(pa1[g,i,t],gamma_pt[i,c]) + pow((1-pa1[g,i,t]),gamma_pt[i,c]) 	#the denominator of the weighting function
	den_w_a2_pt[g,i,c,(t-chunkLength*(c-1))]	= delta_pt[i,c]*pow(pa1[g,i,t],gamma_pt[i,c]) + pow((1-pa1[g,i,t]),gamma_pt[i,c]) 
	den_w_b1_pt[g,i,c,(t-chunkLength*(c-1))]	= delta_pt[i,c]*pow(pb1[g,i,t],gamma_pt[i,c]) + pow((1-pa1[g,i,t]),gamma_pt[i,c])
	den_w_b2_pt[g,i,c,(t-chunkLength*(c-1))]	= delta_pt[i,c]*pow(pb1[g,i,t],gamma_pt[i,c]) + pow((1-pa1[g,i,t]),gamma_pt[i,c])

	w_a1_pt[g,i,c,(t-chunkLength*(c-1))]	= delta_pt[i,c]*pow(pa1[g,i,t],gamma_pt[i,c]) / den_w_a1_pt[g,i,c,(t-chunkLength*(c-1))]		#weightfunction calculated with two parameter function (Goldstein and Einhorn (1987))
	w_a2_pt[g,i,c,(t-chunkLength*(c-1))]	= delta_pt[i,c]*pow((1-pa1[g,i,t]),gamma_pt[i,c]) / den_w_a2_pt[g,i,c,(t-chunkLength*(c-1))]			
	w_b1_pt[g,i,c,(t-chunkLength*(c-1))]	= delta_pt[i,c]*pow(pb1[g,i,t],gamma_pt[i,c]) / den_w_b1_pt[g,i,c,(t-chunkLength*(c-1))]
	w_b2_pt[g,i,c,(t-chunkLength*(c-1))]	= delta_pt[i,c]*pow((1-pb1[g,i,t]),gamma_pt[i,c]) / den_w_b2_pt[g,i,c,(t-chunkLength*(c-1))]

	ev_a_pt[g,i,c,(t-chunkLength*(c-1))]	= u_a1_pt[g,i,c,(t-chunkLength*(c-1))] * w_a1_pt[g,i,c,(t-chunkLength*(c-1))] + u_a2_pt[g,i,c,(t-chunkLength*(c-1))] * w_a2_pt[g,i,c,(t-chunkLength*(c-1))]	#The expectedvalue of the gamble is the utility multiplied with the weight
	ev_b_pt[g,i,c,(t-chunkLength*(c-1))]	= u_b1_pt[g,i,c,(t-chunkLength*(c-1))] * w_b1_pt[g,i,c,(t-chunkLength*(c-1))] + u_b2_pt[g,i,c,(t-chunkLength*(c-1))] * w_b2_pt[g,i,c,(t-chunkLength*(c-1))]

	dev_pt[g,i,c,(t-chunkLength*(c-1))] 	= ev_a_pt[g,i,c,(t-chunkLength*(c-1))] - ev_b_pt[g,i,c,(t-chunkLength*(c-1))]						#difference in expected values 
		
	sdev_pt[g,i,c,(t-chunkLength*(c-1))]	= -1 * beta_pt[i,c] * dev_pt[g,i,c,(t-chunkLength*(c-1))] 						#sensitivity-scaled difference in ev

	tmp_pt[g,i,c,(t-chunkLength*(c-1)] 	= (1)/(1+(exp(sdev_pt[g,i,c,(t-chunkLength*(c-1))]))) 

        theta[g,i,c,(t-chunkLength*(c-1),1] 	= max(0.000001,min(0.999999,tmp_pt[g,i,c,(t-chunkLength*(c-1)])) 			#ensure 0 < cp < 1, accommodates parameter expansion for z
        theta[g,i,c,(t-chunkLength*(c-1),3]  	= max(0.000001,min(0.999999,tmp_pt[g,i,c,(t-chunkLength*(c-1)])) 
        theta[g,i,c,(t-chunkLength*(c-1),5]  	= max(0.000001,min(0.999999,tmp_pt[g,i,c,(t-chunkLength*(c-1)])) 
        theta[g,i,c,(t-chunkLength*(c-1),7] 	= max(0.000001,min(0.999999,tmp_pt[g,i,c,(t-chunkLength*(c-1)])) 

	#-----------lml-model-----------
	u_a1_lml[g,i,c,(t-chunkLength*(c-1))]	= pow(dx1[g,i,t[g,i,c,t]],alpha_lml[i,c])						#prospect utility, computed by exponentiating current wealth by alpha   	
	u_a2_lml[g,i,c,(t-chunkLength*(c-1))]	= pow(dx2[g,i,t[g,i,c,t]],alpha_lml[i,c])	
	u_b1_lml[g,i,c,(t-chunkLength*(c-1))]	= pow(dx3[g,i,t[g,i,c,t]],alpha_lml[i,c])
	u_b2_lml[g,i,c,(t-chunkLength*(c-1))]	= pow(dx4[g,i,t[g,i,c,t]],alpha_lml[i,c])

	tmp_w_a1_lml[g,i,c,(t-chunkLength*(c-1))]	= pa1[g,i,t] + sqrt(pa1[g,i,t]/t) 						#weighfunction (numerator)
	tmp_w_a2_lml[g,i,c,(t-chunkLength*(c-1))]	= (1-pa1[g,i,t]) + sqrt((1-pa1[g,i,t])/t) 
	tmp_w_b1_lml[g,i,c,(t-chunkLength*(c-1))]	= pb1[g,i,t] + sqrt(pb1[g,i,t]/t)
	tmp_w_b2_lml[g,i,c,(t-chunkLength*(c-1))]	= (1-pb1[g,i,t]) + sqrt((1-pb1[g,i,t])/t) 

	w_a1_lml[g,i,c,(t-chunkLength*(c-1))]	= tmp_w_a1_lml[g,i,c,(t-chunkLength*(c-1))] / ( tmp_w_a1_lml[g,i,c,(t-chunkLength*(c-1))] + tmp_w_a2_lml[g,i,c,(t-chunkLength*(c-1))] )	#normalizing the weights
	w_a2_lml[g,i,c,(t-chunkLength*(c-1))]	= tmp_w_a2_lml[g,i,c,(t-chunkLength*(c-1))] / ( tmp_w_a1_lml[g,i,c,(t-chunkLength*(c-1))] + tmp_w_a2_lml[g,i,c,(t-chunkLength*(c-1))] )
	w_b1_lml[g,i,c,(t-chunkLength*(c-1))]	= tmp_w_b1_lml[g,i,c,(t-chunkLength*(c-1))] / ( tmp_w_b1_lml[g,i,c,(t-chunkLength*(c-1))] + tmp_w_b2_lml[g,i,c,(t-chunkLength*(c-1))] )
	w_b2_lml[g,i,c,(t-chunkLength*(c-1))]	= tmp_w_b2_lml[g,i,c,(t-chunkLength*(c-1))] / ( tmp_w_b1_lml[g,i,c,(t-chunkLength*(c-1))] + tmp_w_b2_lml[g,i,c,(t-chunkLength*(c-1))] )
	
	ev_a_lml[g,i,c,(t-chunkLength*(c-1))]	= u_a1_lml[g,i,c,(t-chunkLength*(c-1))] * w_a1_lml[g,i,c,(t-chunkLength*(c-1))] + u_a2_lml[g,i,c,(t-chunkLength*(c-1))] * w_a2_lml[g,i,c,(t-chunkLength*(c-1))]	#The expected value of the gamble is the utility multiplied with the weight
	ev_b_lml[g,i,c,(t-chunkLength*(c-1))]	= u_b1_lml[g,i,c,(t-chunkLength*(c-1))] * w_b1_lml[g,i,c,(t-chunkLength*(c-1))] + u_b2_lml[g,i,c,(t-chunkLength*(c-1))] * w_b2_lml[g,i,c,(t-chunkLength*(c-1))]

	dev_lml[g,i,c,(t-chunkLength*(c-1))] 	= ev_a_lml[g,i,c,(t-chunkLength*(c-1))] - ev_b_lml[g,i,c,(t-chunkLength*(c-1))]						#difference in expected values 

	sdev_lml[g,i,c,(t-chunkLength*(c-1))]	= -1 * beta_lml[i,c] * dev_lml[g,i,c,(t-chunkLength*(c-1))] 					#sensitivity-scaled difference in ev
	
	tmp_lml[g,i,c,(t-chunkLength*(c-1)] 	= (1)/(1+(exp(sdev_lml[g,i,c,(t-chunkLength*(c-1))])))

        theta[g,i,c,(t-chunkLength*(c-1),1] 	= max(0.000001,min(0.999999,tmp_lml[g,i,c,(t-chunkLength*(c-1)])) 			#ensure 0 < cp < 1, accommodates parameter expansion for z
        theta[g,i,c,(t-chunkLength*(c-1),3]  	= max(0.000001,min(0.999999,tmp_lml[g,i,c,(t-chunkLength*(c-1)])) 
        theta[g,i,c,(t-chunkLength*(c-1),5]  	= max(0.000001,min(0.999999,tmp_lml[g,i,c,(t-chunkLength*(c-1)])) 
        theta[g,i,c,(t-chunkLength*(c-1),7] 	= max(0.000001,min(0.999999,tmp_lml[g,i,c,(t-chunkLength*(c-1)])) 	

    	#-----------choice-----------
        y[g,i,c,(t-chunkLength*(c-1))]           	~ dbern(theta[g,i,c,(t-chunkLength*(c-1),z[i]]) 

}#end of trials 
}#end of chunks
}#end of agents
}#end of gambles

##PRIORS

#indicator variables 
#the model indicator variable z can take on any value from 1:n, and is subject to two stochastic processes, to prevent getting stuck
#the n values map onto just 2 models, and is simply a means of obtaining parameter expansion for the model indication

for (i in 1:nAgents){ 
	px_z1[i]    ~ dcat(pz[])                                  #parameter expansion variable for z, takes on integers 1:n with equal probability
	px_z2[i]    ~ dcat(pz[])                                 
	delta_z1[i] = px_z2[i]-1                                  #parameter expansion variable for z, takes on integers 0:n-1 with equal probability
	sum_z[i]    = px_z1[i]+delta_z1[i]                        #sum takes on integers 1:2*n -1 with equal probability
	z[i]        = (sum_z[i] - (8 * trunc(sum_z[i]/8))) + 1    #modulo n, adding 1 to return to values 1 to 8
} 

#submodels
for (i in 1:nAgents){	
for (c in 1:nChunks){

	#-----------pt-----------
	beta_pt[i,c]		= exp(log_beta_pt[i,c])                            #lognormally distributed priors
	log_beta_pt[i,c]	~ dnorm(mu_log_beta_pt[c], tau_log_beta_pt[c])     #log beta_pt sampled from normal dist.

	alpha_pt[i,c]		= exp(log_alpha_pt[i,c])                           #lognormally distributed priors
	log_alpha_pt[i,c]	~ dnorm(mu_log_alpha_pt[c], tau_log_alpha_pt[c])   #log alpha_pt sampled from normal dist.

	delta_pt[i,c]		= exp(log_delta_pt[i,c])                           #lognormally distributed priors
	log_delta_pt[i,c]	~ dnorm(mu_log_delta_pt[c], tau_log_delta_pt[c])   #log delta_pt sampled from normal dist.

	gamma_pt[i,c]		= exp(log_gamma_pt[i,c])                           #lognormally distributed priors
	log_gamma_pt[i,c]	~ dnorm(mu_log_gamma_pt[c], tau_log_gamma_pt[c])   #log gamma_pt sampled from normal dist.	

	#-----------lml-----------
	beta_lml[i,c]		= exp(log_beta_lml[i,c])                           #lognormally distributed priors
	log_beta_lml[i,c]	~ dnorm(mu_log_beta_lml[c], tau_log_beta_lml[c])   #log beta_lml sampled from normal dist.

	alpha_lml[i,c]		= exp(log_alpha_lml[i,c])                          #lognormally distributed priors
	log_alpha_lml[i,c]	~ dnorm(mu_log_alpha_lml[c], tau_log_alpha_lml[c]) #log alpha_lml sampled from normal dist.

}#end of chunks
}#end of agents


##HYPERPRIORS

for (c in 1:nChunks){
	#-----------pt-----------
	mu_log_beta_pt[c]       ~ dunif(muLogBetaL,muLogBetaU)  	#prior on mean of dist. of log beta_pt
	tau_log_beta_pt[c]      = pow(sigma_log_beta_pt[c],-2)   	#prior on precision of dist. of log beta_pt
	sigma_log_beta_pt[c]    ~ dunif(sigmaLogBetaL,sigmaLogBetaU)	#prior on std of dist. of log beta_pt

	mu_log_alpha_pt[c]      ~ dunif(muLogAlphaL,muLogAlphaU)        #prior on mean of dist. of log alpha_pt 
	tau_log_alpha_pt[c]     = pow(sigma_log_alpha_pt[c],-2)         #prior on precision of dist. of log alpha_pt
	sigma_log_alpha_pt[c]   ~ dunif(sigmaLogAlphaL,sigmaLogAlphaU)  #prior on std of dist. of log alpha_pt

	mu_log_delta_pt[c]      ~ dunif(muLogDeltaL,muLogDeltaU)        #prior on mean of dist. of log delta_pt
	tau_log_delta_pt[c]     = pow(sigma_log_delta_pt[c],-2)         #prior on precision of dist. of log delta_pt
	sigma_log_delta_pt[c]   ~ dunif(sigmaLogDeltaL,sigmaLogDeltaU)  #prior on std of dist. of log delta_pt

	mu_log_gamma_pt[c]      ~ dunif(muLogGammaL,muLogGammaU)        #prior on mean of dist. of log gamma_pt
	tau_log_gamma_pt[c]     = pow(sigma_log_gamma_pt[c],-2)         #prior on precision of dist. of log gamma_pt
	sigma_log_gamma_pt[c]   ~ dunif(sigmaLogGammaL,sigmaLogGammaU)  #prior on std of dist. of log gamma_pt

	#-----------lml-----------
	mu_log_beta_lml[c]       ~ dunif(muLogBetaL,muLogBetaU)		#prior on mean of dist. of log beta_lml
	tau_log_beta_lml[c]      = pow(sigma_log_beta_lml[c],-2)   	#prior on precision of dist. of log beta_lml
	sigma_log_beta_lml[c]    ~ dunif(sigmaLogBetaL,sigmaLogBetaU)   #prior on std of dist. of log beta_lml

	mu_log_alpha_lml[c]      ~ dunif(muLogAlphaL,muLogAlphaU)       #prior on mean of dist. of log alpha_pt 
	tau_log_alpha_lml[c]     = pow(sigma_log_alpha_lml[c],-2)       #prior on precision of dist. of log alpha_pt
	sigma_log_alpha_lml[c]   ~ dunif(sigmaLogAlphaL,sigmaLogAlphaU) #prior on std of dist. of log alpha_pt
}#end of chunks
}